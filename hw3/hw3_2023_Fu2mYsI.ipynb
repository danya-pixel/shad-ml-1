{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recorded-outside",
   "metadata": {},
   "source": [
    "# Машинное обучение, ШАД\n",
    "## Лабораторная работа 3. Линейные модели классификации и регрессии, валидация моделей.\n",
    "\n",
    "\n",
    "**Правила:**\n",
    "\n",
    "* Выполненную работу нужно отправить в соответствующее задание в личном кабинете\n",
    "* Дедлайн **5 октября 17:00 НСК**. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Для сдачи задания нужно загрузить **ноутбук в формате `ipynb`** в ЛМС.\n",
    "* Выполнять задание необходимо полностью самостоятельно.\n",
    "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него. Можно добавлять необходимое количество ячеек.\n",
    "* Комментарии к решению пишите в markdown-ячейках.\n",
    "* Выполнение задания (ход решения, выводы и пр.) должно быть осуществлено на русском языке.\n",
    "* Присылайте понятный и читаемый код. Если код не будет понятен проверяющему, оценка может быть снижена.\n",
    "* Код из данного задания при проверке запускаться не будет. *Если код студента не выполнен, недописан и т.д., то он не оценивается.*\n",
    "\n",
    "\n",
    "**Правила оформления теоретических задач:**\n",
    "\n",
    "* Решения необходимо прислать одним из следующих способов:\n",
    "  * фотографией в правильной ориентации, где все четко видно, а почерк разборчив,\n",
    "    * прикрепив ее в ЛМС в форматах `pdf`, `png` или `jpg` *или*\n",
    "    * вставив ее в ноутбук посредством `Edit -> Insert Image`;\n",
    "  * в виде $\\LaTeX$ в markdown-ячейках или в отдельном `pdf`-файле.\n",
    "* Решения не проверяются, если какое-то требование не выполнено. Особенно внимательно все проверьте в случае выбора второго пункта (вставки фото в ноутбук). <font color=\"red\"><b>Неправильно вставленные фотографии могут не передаться при отправке.</b></font> Для проверки попробуйте переместить `ipynb` в другую папку и открыть его там.\n",
    "* В решениях поясняйте, чем вы пользуетесь, хотя бы кратко. Например, если пользуетесь независимостью, то достаточно подписи вида \"*X и Y незав.*\"\n",
    "* Решение, в котором есть только ответ, и отсутствуют вычисления, оценивается в 0 баллов.\n",
    "\n",
    "**Баллы за задание:**\n",
    "\n",
    "* Задача 1 &mdash; 1 балл;\n",
    "* Задача 2 &mdash; 1 балл;\n",
    "* Задача 3 &mdash; 1 балл;\n",
    "* Задача 4 &mdash; 1 балл;\n",
    "* Задача 5 &mdash; 5 баллов.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc297b-000f-4ee5-9ca5-0ccdeba20c1d",
   "metadata": {},
   "source": [
    "---\n",
    "### Задача 1. \n",
    "\n",
    "1. Пусть $X_1,...,X_n$ &mdash; выборка из гамма-распределения с плотностью $p_\\theta(x) = \\frac{\\theta^\\beta}{\\Gamma(\\beta)} x^{\\beta-1} e^{-\\theta x}$, где $\\theta>0, \\beta>0, x>0$, причем $\\beta$ известно. Найдите оценку максимального правдоподобия параметра $\\theta$.\n",
    "\n",
    "2. Пусть $X_1,...,X_n$ &mdash; выборка из пуассоновского распределения, для которого $\\mathsf{P}_\\theta(X_i = k) = \\frac{\\theta^k}{k!}e^{-\\theta}$, где $\\theta>0, k \\in \\mathbb{Z}_+ = \\{0, 1, 2, ...\\}$. Найдите оценку максимального правдоподобия параметра $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647059e",
   "metadata": {},
   "source": [
    "**Задача 1.1**\n",
    "\n",
    "Построим логарифмическую функцию правдоподобия:\n",
    "$$\n",
    "l_{x}(\\theta) = \\sum_{i=1}^n \\ln p_{\\theta}(x)= n \\beta \\ln \\theta-n \\ln \\Gamma(\\theta) + (\\beta-1) \\sum_{i=1}^n\\ln x_{i} - \\theta \\sum_{i=1}^n x_{i}\n",
    "$$\n",
    "\n",
    "Найдем максимум:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial l_{x} }{ \\partial \\theta } = \\frac{n\\beta}{\\theta} - \\sum_{i=1}^n x_{i} =0\n",
    "$$\n",
    "\n",
    "Откуда:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{n\\beta}{\\sum_{i=1}^n x_{i}} = (\\overline{x})^{-1}\\beta\n",
    "$$\n",
    "\n",
    "\n",
    "**Задача 1.2**\n",
    "\n",
    "Построим логарифмическую функцию правдоподобия:\n",
    "\n",
    "$$\n",
    "l_{x}(\\theta) = \\sum_{i=1}^n (x_{i}\\ln\\theta-\\ln x_{i}!-\\theta)=\\ln \\theta \\sum_{i=1}^n x_{i}- \\sum_{i=1}^n \\ln x_{i}! - n\\theta\n",
    "$$\n",
    "\n",
    "Ищем максимум\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial l_{x} }{ \\partial \\theta } = \\frac{\\sum_{i=1}^n x_{i}}{\\theta} - n =0\n",
    "$$\n",
    "\n",
    "Откуда\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\overline{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd19ad3-9765-489d-985a-1010a6f2145e",
   "metadata": {},
   "source": [
    "---\n",
    "### Задача 2.\n",
    "\n",
    "Пусть $\\ell_Y(\\theta)$ &mdash; логарифмическая функция правдоподобия в модели логистической регрессии, задачу максимизации которой мы рассматривали на лекции. Добавим к ней $L_2$-регуляризатор и будем рассматривать задачу\n",
    "$$F(\\theta) = -\\ell_Y(\\theta) + \\lambda \\|\\theta\\|^2  \\longrightarrow \\min_{\\theta \\in \\mathbb{R}^d},$$\n",
    "где $\\lambda > 0$ &mdash; коэффициент регуляризации.\n",
    "\n",
    "1. Выпишите формулы градиентного спуска (GD) и стохастического градиентного спуска (SGD).\n",
    "\n",
    "2. Покажите, что $F(\\theta)$ &mdash; выпуклая функция по $\\theta$ и, как следствие, имеет единственный экстремум, являющийся глобальным максимумом. *Указание*. Посчитайте гессиан (матрицу вторых производных) и покажите, что она положительно определена.\n",
    "\n",
    "3. Опишите, как может вести себя решение при отсутствии регуляризации, то есть при $\\lambda = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622601ad",
   "metadata": {},
   "source": [
    "#### Задача 2.1\n",
    "\n",
    "Выпишем градиент $F(\\theta)$:\n",
    "\n",
    "$$ \\nabla F (\\theta) =\n",
    "- \\sum_{i=1}^n x_{i} (y_{i}-\\sigma(\\langle  \\theta, x_{i}\\rangle )) + 2 \\lambda \\theta\n",
    "$$\n",
    "\n",
    "**GD**\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta[\\sum_{i=1}^n x_{i}(\\sigma(\\langle \\theta_{t}, x_{i} \\rangle - y_{i})) + 2\\lambda\\theta_{t} ]\n",
    "$$\n",
    "В матричном виде:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta [X^T (s(\\theta_{t}) - Y)+2\\lambda\\theta_{t}]\n",
    "$$\n",
    "где $\\vec{s}(\\theta) = (\\sigma(\\theta^Tx_{i}))$ – вектор – столбец.\n",
    "\n",
    "**SGD**\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta\\left[ \\frac{n}{k}\\sum_{i \\in I} x_{i}(\\sigma(\\langle \\theta_{t}, x_{i} \\rangle - y_{i})) + 2\\lambda\\theta_{t}  \\right]\n",
    "$$\n",
    "где $I = \\{ i_{1}, \\dots i_{k} \\} \\sim \\cup \\{ 1, \\dots n \\}$.\n",
    "\n",
    "В матричном виде;\n",
    "\n",
    "$$\n",
    "\\theta_{t+1}= \\theta_{t} + \\eta [\\frac{n}{k} X^T(Y-S(\\theta_{t})_{I}) + 2\\lambda\\theta_{t}]\n",
    "$$, где $X_{i}$ – строки матрицы $X$ соответствующие индексам $I$,\n",
    "$Y_{i}$ – элементы вектора $Y$, соответствующие индексам $I$,\n",
    "$S(\\theta_{t})$ – аналогично.\n",
    "\n",
    "#### Задача 2.2\n",
    "\n",
    "$$F(\\theta) = -\\ell_Y(\\theta) + \\lambda \\|\\theta\\|^2 $$\n",
    "Выпишем градиент $F(\\theta)$:\n",
    "$$ \\nabla F (\\theta) =\n",
    "- \\sum_{i=1}^n x_{i} (y_{i}-\\sigma(\\langle  \\theta, x_{i}\\rangle )) + 2 \\lambda \\theta\n",
    "$$\n",
    "$j$ - я компонента градиента имеет следующий вид:\n",
    "\n",
    "$$\n",
    "\\nabla F(\\theta)_{j} = - \\sum_{i=1}^n x_{ij}(y_{i}-\\sigma(\\langle \\theta, x_{i} \\rangle)) + 2\\lambda\\theta_{j} \n",
    "$$\n",
    "\n",
    "Чтобы получить гессиан, выпишем производную по $k$-й компоненте $\\theta$:\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial^2  }{ \\partial \\theta_{j}\\theta_{k} } F(\\theta)=F(\\theta)_{jk} = \\sum_{i=1}^n (x_{ij}x_{ik}\\sigma(\\langle \\theta, x_{i} \\rangle )(1-\\sigma(\\langle \\theta, x_{i} \\rangle ))) + 2 \\delta_{jk} \\lambda\n",
    "$$\n",
    ", где $\\delta_{jk}$ – символ Кронекера.\n",
    "\n",
    "Обозначим за $D$ диагональную матрицу с элементами $\\sigma(\\langle \\theta, x_{i} \\rangle)(1-\\sigma(\\langle \\theta, x_{i} \\rangle))$, $I$ – единичная матрица. Тогда гессиан можно представить в виде:\n",
    "$$\n",
    "\\nabla ^2 F = X^TDX+2\\lambda E\n",
    "$$\n",
    "Так как $0 < \\sigma(\\langle \\theta, x_{i} \\rangle) < 1$, на диагонали стоят положительные числа, из которых можно извлечь квадратные корни, представив $D$ в виде $D = D^{1/2}D^{1/2}$. Матрица $X$ имеет полный ранг по столбцам, а матрица $2\\lambda E$ – тоже диагональная с положительными числами. Тогда, для любого приращения $u\\neq 0$ имеем\n",
    "\n",
    "$$\n",
    "u^T X^T DXu = u^TX^T(D^{1/2})^TD^{1/2}Xu = | D^{1/2}Xu|^2 >0\n",
    "$$\n",
    "Таким образом, функция $F$ выпукла вниз как функция от $\\theta$, и, соответственно, точка её экстремума непременно будет точкой минимума (если точка экстремума существует). \n",
    "\n",
    "#### Задача 2.3\n",
    "Без регуляризации, модель логистической регрессии будет склонна к переобучению. Это означает, что параметры $\\theta$ будут настроены таким образом, чтобы минимизировать ошибку на обучающих данных, но при этом модель может быть неспособной к обобщению на новые данные. Можно рассмотреть пример, когда обучающая выборка идеально разделима и мы работаем в $\\mathbb{R}^2$. Представим, что данные не пересекаются и разделяются вертикальной линией $x=0$. Запишем логарифмическую ФМП:\n",
    "$$\n",
    "l_{y}(\\theta)= \\sum_{i=1}^n [y_{i}\\log \\sigma(\\theta^Tx_{i})+(1-y_{i})\\log(1-\\sigma(\\theta^Tx_{i}))]\n",
    "$$\n",
    "При $y_{i}=0$ рассматриваем член $\\log(1-\\sigma(\\theta^Tx_{i}))$\n",
    "При $y_{i}=1$ рассматриваем член $\\log \\sigma(\\theta^Tx_{i})$.\n",
    "\n",
    "В итоге, при \n",
    "$$\n",
    "\\lim_{ \\theta \\to \\infty } \\log(1-\\sigma(\\theta^Tx_{i})) =0\n",
    "$$\n",
    "$$\n",
    "\\lim_{ \\theta \\to \\infty } \\log \\sigma(\\theta^T x_{i}) = 0\n",
    "$$\n",
    "Получаем, что $\\theta$ монотонно возрастает. У целевой функции нет максимума, поэтому, процесс оптимизации привет к устремлению весов в бесконечность. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4775a-a8a4-43f4-960d-d96702fe9254",
   "metadata": {},
   "source": [
    "---\n",
    "### Задача 3.\n",
    "\n",
    "Рассмотрим линейную регрессию $y(x) = x^T \\theta$, причем для оценки $\\theta$ будем рассматривать функцию потерь Хьюбера\n",
    "$$R(x) = \\frac{x^2}{2} I\\{|x| \\leqslant c\\} + c\\left(|x| - \\frac{c}{2}\\right)I\\{|x| > c\\}.$$\n",
    "\n",
    "Тем самым задача оптимизации имеет вид\n",
    "$$\\sum_{i=1}^n R(Y_i + x_i^T \\theta) \\longrightarrow \\min_{\\theta \\in \\mathbb{R}^d}.$$\n",
    "\n",
    "1. Нарисуйте график $R(x)$. В чем польза выбора такой функции потерь?\n",
    "\n",
    "2. Выпишите формулы градиентного спуска (GD) и стохастического градиентного спуска (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91384258",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdc323",
   "metadata": {},
   "source": [
    "По сути, функция потерь Хьюбера – комбинация MSE и MAE (опуская среднее значение), она квадратична для малых x, линейна для больших x. Иными словами, мы получаем что-то среднее, между сильной чувствительностью к выбросам (MSE) и игнорированием выбросов (MAE). Такая функция потерь хороша сработает, когда мы хотим дать небольшой, сердний вес выбросам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea598c",
   "metadata": {},
   "source": [
    "**GD**:\n",
    "$$\n",
    "\\theta_{t+1}= \\theta_{t} - \\eta\\left( \\frac{1}{2}X^T(X\\theta_{t}-y)I\\{ X\\theta_{t}-y\\leq c \\} + cX^Tsign(X\\theta_{t}-y)I\\{ X\\theta_{t}-y > c \\}\\right)\n",
    "$$\n",
    "\n",
    "**SGD**:\n",
    "$$\n",
    "\\theta_{t+1}= \\theta_{t} - \\eta\\left( \\frac{n}{k}\\sum_{i \\in I}\\left[ \\frac{1}{2}x_{i}(x_{i}^T\\theta_{t}-y)I\\{ X\\theta_{t}-y \\leq c \\} + c x_i sign(x_i^T\\theta_{t}-y_i)I\\{ X\\theta_{t}-y > c \\} \\right]\\right)\n",
    "$$, где $I = \\{ i_{1}, \\dots i_{k} \\} \\sim \\cup \\{ 1, \\dots n \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18379a-ad1c-4fa6-97e7-964b9cbc3238",
   "metadata": {},
   "source": [
    "---\n",
    "### Задача 4.\n",
    "\n",
    "В предыдущем домашнем задании вы подробно познакомились с линейными моделями, выяснили о необходимости обработки непрерывных и категориальных признаков, узнали о способах подбора гиперпараметров. Порассуждайте над следующими вопросами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7354a50-8c39-46bc-875c-be3de71c6c66",
   "metadata": {},
   "source": [
    "**1.** Какие побочные эффекты могут возникунуть при стандартизации (нормализации) признаков с помощью `StandardScaler`, `MinMaxScaler`? Что с этим можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a4f00-0342-43bb-ac02-8aa56940ba63",
   "metadata": {},
   "source": [
    "**2.** Рассмотрим пример с первого занятия про модель потребления мороженного от температуры:\n",
    "$$ic=\\theta_1 + \\theta_2 t.$$\n",
    "Предположим, что нам также известен еще один признак, отвечающий за год. Обозначим его за $y$. Пусть $y \\in \\{1, 2, 3\\}$. Попробуем учесть влияние года двумя разными способами:\n",
    "* Модель $ic = \\theta_1 + \\theta_2 t+ \\theta_3 y_1 + \\theta_4 y_2$, где $y_1 = I\\{y=1\\}$, $y_2 = I\\{y=2\\}$.\n",
    "* Для каждого года рассматривается своя линейная зависимость $ic=\\theta_1+\\theta_2 t$.\n",
    "  \n",
    "Поясните, в чем различие этих двух подходов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558fc9e-da13-4ec8-9841-aaa742c454e6",
   "metadata": {},
   "source": [
    "**3.** Визуализируйте совместные распределения вещественных признаков и целевой переменной для данных из предыдущего домашнего задания. Что можно сказать о зависимости таргета от признаков? Сделайте вывод о том, насколько хорошо построенные модели приближают истинные зависимости. \n",
    "\n",
    "Полученные графики приложите к решению теоретического задания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-gnome",
   "metadata": {},
   "source": [
    "---\n",
    "### Задача 5.\n",
    "\n",
    "Реализуйте логистическую регрессию с $L_2$ регуляризацией для поиска оценки параметров с помощью стохастического mini-batch градиентного спуска (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34757792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "interested-backup",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4xxxebKnQLm"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Модель логистической регрессии. Имеет следующие гиперпараметры:\n",
    "\n",
    "    :param alpha: параметр регуляризации. \n",
    "                  Если равно 0, то регуляризация не происходит.\n",
    "    :param lr: константа, на которую домножаем градиент при обучении\n",
    "    :param max_iter: ограничение на кол-во итераций\n",
    "    :param fit_intercept: указывает, следует ли добавить константу в признаки\n",
    "    '''\n",
    "\n",
    "    def __init__(self, alpha=0, lr=0.5, tol=1e-7, max_steps=1500, batch_size=64,\n",
    "                 fit_intercept=True, verbose=False):\n",
    "        '''Создает модель и инициализирует параметры.'''\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = 42\n",
    "        self.max_steps = max_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.classes_ = np.array([0, 1])\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return expit(x)\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        '''\n",
    "        Добавляем свободный коэффициент к нашей модели. \n",
    "        Это происходит путем добавления вектора из 1 к исходной матрице.\n",
    "\n",
    "        :param X: исходная матрица признаков\n",
    "        :return: матрица X с добавленным свободным коэффициентов\n",
    "        '''\n",
    "\n",
    "        X_copy = np.full((X.shape[0], X.shape[1] + 1), fill_value=1.)\n",
    "        X_copy[:, :-1] = X\n",
    "\n",
    "        return X_copy\n",
    "    \n",
    "    def _initialize_weights(self, n_features):\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.W = rng.standard_normal(n_features)\n",
    "\n",
    "    def _batch_indexes(self, shape):\n",
    "        indexes = np.arange(shape)\n",
    "        np.random.shuffle(indexes)\n",
    "        return indexes\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        predictions = self._sigmoid(np.dot(self.W, X.T))\n",
    "        loss_1 = y.T.dot(np.log(predictions))\n",
    "        loss_2 = (1-y).T.dot(np.log(1-predictions))\n",
    "        loss = -((loss_1 + loss_2))/y.shape[0] + self.alpha * np.sum(self.W ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_gradients(self, X_batch, y_batch):\n",
    "        predictions = self._sigmoid(np.dot(self.W, X_batch.T))\n",
    "        errors = predictions - y_batch\n",
    "        regularization_grad = 2 * self.alpha * self.W\n",
    "        regularization_grad[-1] = 0\n",
    "        grad = X_batch.T.dot(errors)/len(X_batch) - regularization_grad\n",
    "        return grad\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        Обучает модель логистической регресии с помощью SGD,\n",
    "        пока не выполнится self.max_iter итераций.\n",
    "\n",
    "        :param X: матрица признаков\n",
    "        :param Y: истинные метки\n",
    "        '''\n",
    "\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        if self.fit_intercept:  # добавляем свободный коэфициент\n",
    "            X_copy = self._add_intercept(X)\n",
    "        else:\n",
    "            X_copy = X.copy()\n",
    "\n",
    "        self._initialize_weights(X_copy.shape[1])\n",
    "        in_size = X_copy.shape[0]\n",
    "        prev_loss = float(\"inf\")\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            shuffle_indexes = self._batch_indexes(in_size)\n",
    "\n",
    "            for i in range(0, in_size, self.batch_size):\n",
    "                idx = shuffle_indexes[i:(i + self.batch_size)]\n",
    "                X_batch = X_copy[idx]\n",
    "                y_batch = Y[idx]\n",
    "                grad = self._compute_gradients(X_batch, y_batch)\n",
    "                self.W -= self.lr * grad\n",
    "\n",
    "            \n",
    "            if self.tol is not None:\n",
    "                loss = self._compute_loss(X_copy, Y)\n",
    "                if self.verbose:\n",
    "                    print(f\"step: {step}, loss: {loss}\")\n",
    "                if prev_loss - loss < self.tol:\n",
    "                    break\n",
    "                prev_loss = loss\n",
    "\n",
    "        self.coef_ = self.W[:-1]\n",
    "        self.intercept_ = self.W[-1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Применяет обученную модель к данным \n",
    "        и возвращает точечное предсказание (оценку класса).\n",
    "\n",
    "        :param X: матрица признаков\n",
    "        :return: предсказание с размерностью (n_test, )\n",
    "        '''\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X_copy = self._add_intercept(X)\n",
    "        else:\n",
    "            X_copy = X.copy()\n",
    "        \n",
    "        assert X_copy.shape[1] == self.W.shape[0]\n",
    " \n",
    "        predictions = self._sigmoid(np.dot(self.W, X_copy.T))\n",
    "\n",
    "        return [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Применяет обученную модель к данным\n",
    "        и возвращает предсказание вероятности классов 0 и 1.\n",
    "\n",
    "        :param X: матрица признаков\n",
    "        :return: вероятности предсказания с размерностью (n_test, 2)\n",
    "        '''\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X_copy = self._add_intercept(X)\n",
    "        else:\n",
    "            X_copy = X.copy()\n",
    "\n",
    "        assert X_copy.shape[1] == self.W.shape[0]\n",
    "\n",
    "        prob_predictions = self._sigmoid(np.dot(self.W, X_copy.T))\n",
    "\n",
    "        return prob_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-adrian",
   "metadata": {},
   "source": [
    "Рассмотрим игрушечный датасет на $30$ признаков `load_breast_cancer` из библиотеки `sklearn`. Это относительно простой для бинарной классификации датасет по диагностике рака молочной железы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-request",
   "metadata": {
    "colab_type": "text",
    "id": "HHPTpzcWhv_W"
   },
   "source": [
    "Ради интереса можно прочитать описание признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "anonymous-raising",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "uUMbGPj-Fgfi",
    "outputId": "9b6b8f4b-d90a-42d7-d6d7-880c5853a33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    :Attribute Information:',\n",
       " '        - radius (mean of distances from center to points on the perimeter)',\n",
       " '        - texture (standard deviation of gray-scale values)',\n",
       " '        - perimeter',\n",
       " '        - area',\n",
       " '        - smoothness (local variation in radius lengths)',\n",
       " '        - compactness (perimeter^2 / area - 1.0)',\n",
       " '        - concavity (severity of concave portions of the contour)',\n",
       " '        - concave points (number of concave portions of the contour)',\n",
       " '        - symmetry',\n",
       " '        - fractal dimension (\"coastline approximation\" - 1)',\n",
       " '',\n",
       " '        The mean, standard error, and \"worst\" or largest (mean of the three',\n",
       " '        worst/largest values) of these features were computed for each image,',\n",
       " '        resulting in 30 features.  For instance, field 0 is Mean Radius, field',\n",
       " '        10 is Radius SE, field 20 is Worst Radius.',\n",
       " '',\n",
       " '        - class:',\n",
       " '                - WDBC-Malignant',\n",
       " '                - WDBC-Benign']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "dataset['DESCR'].split('\\n')[11:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-intersection",
   "metadata": {
    "colab_type": "text",
    "id": "JgaXPncW-Gab"
   },
   "source": [
    "Разделим нашу выборку на обучающую и тестовую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "otherwise-savage",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WEn6HImRc8zJ",
    "outputId": "9c25a5a2-4ea6-4e33-c9be-b780470fbbbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30), (455,), (114,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = dataset['data'], dataset['target']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-process",
   "metadata": {
    "colab_type": "text",
    "id": "l8jzwZUCPB_l"
   },
   "source": [
    "При использовании регуляризации данные необходимо нормализовать. Воспользуемся для этого классом `StandardScaler` из библиотеки `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "backed-substance",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNAqhHbZPBvb"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-break",
   "metadata": {},
   "source": [
    "Теперь обучите модель логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7cf12b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.21478718803226263\n",
      "step: 1, loss: 0.1680282270801709\n",
      "step: 2, loss: 0.14217401922686906\n",
      "step: 3, loss: 0.12496120944509347\n",
      "step: 4, loss: 0.1218402913947143\n",
      "step: 5, loss: 0.10667311441741889\n",
      "step: 6, loss: 0.1023646591948976\n",
      "step: 7, loss: 0.10224880921367713\n",
      "step: 8, loss: 0.0970825047539878\n",
      "step: 9, loss: 0.09398567267170417\n",
      "step: 10, loss: 0.0906693210919656\n",
      "step: 11, loss: 0.08871963183587074\n",
      "step: 12, loss: 0.08620431798686752\n",
      "step: 13, loss: 0.08411650131670216\n",
      "step: 14, loss: 0.08227706149523897\n",
      "step: 15, loss: 0.08062424532087076\n",
      "step: 16, loss: 0.07918327065358693\n",
      "step: 17, loss: 0.07862876831887755\n",
      "step: 18, loss: 0.07661072416442304\n",
      "step: 19, loss: 0.07530340528369207\n",
      "step: 20, loss: 0.07427049722454185\n",
      "step: 21, loss: 0.07318438571712484\n",
      "step: 22, loss: 0.07178546006108212\n",
      "step: 23, loss: 0.07086930400212334\n",
      "step: 24, loss: 0.07001735545171621\n",
      "step: 25, loss: 0.06942536363411664\n",
      "step: 26, loss: 0.06888516945705447\n",
      "step: 27, loss: 0.06766287774044141\n",
      "step: 28, loss: 0.0664778325152385\n",
      "step: 29, loss: 0.06581738653144807\n",
      "step: 30, loss: 0.06520658261124067\n",
      "step: 31, loss: 0.06689967579625132\n",
      "accuracy:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(verbose=True))\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "test_preds = pipe.predict(X_test)\n",
    "print(\"accuracy: \", accuracy_score(Y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-guidance",
   "metadata": {},
   "source": [
    "На занятии обсуждали, что в нашей постановке задачи при сравнении выиграет модель с меньшим FN, ведь каждая не обнаруженная опухоль может стоить человеческой жизни. Чем меньше ложно отрицательных срабатываний, тем выше Recall модели, а значит разумно взять Recall в качестве целевой метрики. \n",
    "\n",
    "Построить модель с Recall = 1 довольно просто (Как?), но в ней не будет большого смысла, т.к., например, для нашей задачи отправление на доп. обследование может стоить дополнительных средств и времени специалистов, поэтому хотелось, чтобы наша модель имела неплохую точность. Какую метрику можно использовать, чтобы учесть и точность, и полноту?\n",
    "\n",
    "Ответ: F1-score!\n",
    "\n",
    "Выберите и посчитайте целевые метрики для нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sweet-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97        43\n",
      "           1       0.99      0.97      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-palestinian",
   "metadata": {
    "colab_type": "text",
    "id": "mE0rZ7vPCH_S"
   },
   "source": [
    "Рассмотрите как влияет размер шага (`learning rate`) на качество модели. Обучите каждую модель одинаковое число итераций (например, 10000), а затем посчитайте качество. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "automotive-partner",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL0NzUTDbuxW"
   },
   "outputs": [],
   "source": [
    "lrs = [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.3, 0.5, 0.7, 1, 2, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cooked-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1e-05, f1: 0.6490066225165563\n",
      "lr 0.0001, f1: 0.7724137931034482\n",
      "lr 0.001, f1: 0.965034965034965\n",
      "lr 0.01, f1: 0.9859154929577465\n",
      "lr 0.1, f1: 0.9859154929577465\n",
      "lr 0.2, f1: 0.9787234042553192\n",
      "lr 0.3, f1: 0.9787234042553192\n",
      "lr 0.5, f1: 0.9787234042553192\n",
      "lr 0.7, f1: 0.9787234042553192\n",
      "lr 1, f1: 0.9714285714285714\n",
      "lr 2, f1: 0.948905109489051\n",
      "lr 5, f1: 0.9565217391304348\n",
      "lr 10, f1: 0.9420289855072463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for lr in lrs:\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(tol=None, max_steps=1000, lr=lr))\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    test_preds = pipe.predict(X_test)\n",
    "    print(f\"lr {lr}, f1: {f1_score(Y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06a100",
   "metadata": {},
   "source": [
    "Лучше всего модель обучается с $lr \\in [0.01, 0.1]$. При больших $lr$ возникают проблемы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-madness",
   "metadata": {
    "colab_type": "text",
    "id": "UQy0zIrcClfm"
   },
   "source": [
    "Рассмотрите несколько моделей, в которых установите не менее 5-ти различных коэффициентов регуляризации, а также модель без регуляризатора. Сравните, влияет ли наличие регуляризации на качество, сделайте выводы. Под качеством подразумевается значение какой-либо выбранной вами метрики качества классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "convertible-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 1, f1: 0.6115702479338843\n",
      "alpha 0.5, f1: 0.628099173553719\n",
      "alpha 0.1, f1: 0.948905109489051\n",
      "alpha 0.2, f1: 0.9481481481481481\n",
      "alpha 0.01, f1: 0.9859154929577465\n",
      "alpha 0.05, f1: 0.9787234042553192\n",
      "alpha 0.001, f1: 0.9859154929577465\n",
      "alpha 0, f1: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "regs = [1, 0.5, 1e-1, 2e-1, 1e-2, 5e-2, 1e-3, 0]\n",
    "for reg in regs:\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(alpha=reg, tol=None, max_steps=20, lr=lr))\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    test_preds = pipe.predict(X_test)\n",
    "    print(f\"alpha {reg}, f1: {f1_score(Y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc041e",
   "metadata": {},
   "source": [
    "`Вывод: небольшое добавление регуляризации улучшает качество, значения ближе к 0.1 и большие – ухудшают качество, модель слишком сильно смотрит на важность регуляризационного члена`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-expense",
   "metadata": {},
   "source": [
    "Выберите наилучшее значение коэффициента регуляризации с помощью кросс-валидации для двух подходов &mdash; `KFold` и `ShuffleSplit`. Используйте пять фолдов/разбиений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "assisted-rouge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold:\n",
      "alpha 1, f1: 0.6459248715520497\n",
      "alpha 0.5, f1: 0.8200177304964539\n",
      "alpha 0.1, f1: 0.9507707234724894\n",
      "alpha 0.2, f1: 0.9215259275820286\n",
      "alpha 0.01, f1: 0.9677172250644677\n",
      "alpha 0.05, f1: 0.9584981096780755\n",
      "alpha 0.001, f1: 0.9760843487001027\n",
      "alpha 0, f1: 0.9705268286276425\n",
      "\n",
      "ShuffleSplit\n",
      "alpha 1, f1: 0.6334975865021424\n",
      "alpha 0.5, f1: 0.7777844482480079\n",
      "alpha 0.1, f1: 0.9372634317111516\n",
      "alpha 0.2, f1: 0.8966935540754927\n",
      "alpha 0.01, f1: 0.9752324231190095\n",
      "alpha 0.05, f1: 0.9507076058190339\n",
      "alpha 0.001, f1: 0.9752324231190095\n",
      "alpha 0, f1: 0.9759147374364765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, ShuffleSplit\n",
    "\n",
    "print(\"KFold:\")\n",
    "for reg in regs:\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(alpha=reg, tol=None, max_steps=20, lr=lr))\n",
    "    kf = KFold(n_splits=5)\n",
    "    print(f\"alpha {reg}, f1: {np.mean(cross_val_score(pipe, X, Y, cv=kf, scoring='f1'))}\")\n",
    "\n",
    "print(\"\\nShuffleSplit\")\n",
    "for reg in regs:\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(alpha=reg, tol=None, max_steps=20, lr=lr))\n",
    "    kf = ShuffleSplit(n_splits=5, random_state=42)\n",
    "    print(f\"alpha {reg}, f1: {np.mean(cross_val_score(pipe, X, Y, cv=kf, scoring='f1'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6e284",
   "metadata": {},
   "source": [
    "`Лучше всего себя показывает alpha = 0.001, хотя, ее значение и метрики близки к модели без использования регуляризации`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dfe74-d6e3-480e-a212-dbf0fbf93fe7",
   "metadata": {},
   "source": [
    "Для выбранного значения коэффициента регуляризации оцените дисперсию усредненного значения метрики качества на тестовых батчах. Для этого выполните кросс-валидацию достаточно много раз (не менее 100) и посчитайте выборочную дисперсию. Обратите внимание, что для стратегии `KFold` нужно на каждой итерации перемешивать данные, для этого можно указать `shuffle=True`.\n",
    "\n",
    "Сравните эти две стратегии кросс-валидации. Какие их преимущества и недостатки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92e1a734-b563-4bea-bcc9-f295fc14e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.046966580848326e-06\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "\n",
    "stats = []\n",
    "\n",
    "for _ in range(150):\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(alpha=reg, tol=None, max_steps=20, lr=lr))\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    stats.append(np.mean(cross_val_score(pipe, X_test, Y_test, cv=kf, scoring='f1')))\n",
    "\n",
    "print(np.var(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6731df9",
   "metadata": {},
   "source": [
    "Дисперсия достаточно мала, потому можно утверждать, что значения метрики полученные при первом подходе валидны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-syntax",
   "metadata": {},
   "source": [
    "**Вывод:** Первый подход позволяет подобрать оптимальные параметры, оценить обобщающую способность модели с разными параметрами. Кросс-валидация с KFold обычно более стабильна и надежна, чем одноразовое разделение данных на обучающий и тестовый наборы. ShuffleSplit может быть полезным, если есть особенности данных, которые можно лучше моделировать случайными разбиениями.\n",
    "\n",
    "Второй подход позволяет оценить уровень дисперсии модели при разных случайных разбиениях данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
